# KNNにおける類似度と誤差の雑メモ

## 1. KNNは完全一致では判定しない
- 画像やデータが **ピッタリ同じ** である必要はない
- KNNは「似ているかどうか」を距離で判断して分類する

---

## 2. 画像をベクトルで表す
- 画像1枚 → 数値の列（ベクトル）に変換
  - ピクセル値を並べる
  - 明度ヒストグラムを使うこともある
- 例：1枚の画像を256次元のベクトルで表す

---

## 3. 距離で似ているか判定
- ユークリッド距離などを使ってベクトル間の距離を計算
- 距離が小さいほど「似ている」
- Python例：
```python
from numpy.linalg import norm

distance = norm(test_vector - train_vector)
print(distance)
```
- 数式で表すと：<br>
![説明](https://github.com/n-masashi/study_notes/blob/main/programming/images/euclidean-distance.png)
- 距離が小さいほど学習画像に近い → 同じラベルを予測

---

## 4. k近傍法 (KNN) の動き
- k=1 → 最も近い1枚の画像のラベルを使う
- k>1 → 近い k 枚の多数決で決定
- 距離が自然に「誤差の範囲」を決めるイメージ

---

## 5. k の値と正答率の傾向
- 小さい k (1〜3)
  - ノイズや例外に敏感
  - 過学習しやすいが、学習データに近ければ正答率高め
- 中くらい k (5〜15)
  - ノイズに強く、精度が安定しやすい
- 大きい k
  - 特定のサンプルへの感度が低くなる
  - 精度は低下しやすい
- 目安として：
  - k ≈ √(サンプル数) を試す
  - 実際は複数の k を試して交差検証で最適な値を決定

---

## 6. ポイントまとめ
1. 完全一致ではなく「似ているか」で判定
2. 距離が近いほど信頼度が高い
3. k の値や距離の計算方法で分類精度が変わる
4. 明度ヒストグラムなど特徴量の選び方で距離の意味が変わる
5. 適切な k は経験的に試す

---
